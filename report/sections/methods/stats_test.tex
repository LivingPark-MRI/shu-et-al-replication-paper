We measured the statistical significance of our models by comparing them to random classifiers to determine if models with an AUC over 0.5 were significant. Additionally, we used permutation testing, running permutations with scikit-learn's `permutation\_test\_score` function (100 times) to validate our cross-validation results. This approach helped us mitigate the risk of misleading conclusions. Our choice of permutation tests over traditional t-tests was motivated by their ability to better account for data variability, as highlighted by \cite{Nadeau_2003}. Finally, we assessed significance at different levels: 5\%, 1\%, and 0.1\%. This gave us a clear picture of how confident we could be in our findings, making our conclusions more reliable.